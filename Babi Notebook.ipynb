{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"tasks_1-20_v1-2/en/qa1_single-supporting-fact_train.txt\", 'r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 Mary moved to the bathroom.\\n',\n",
       " '2 John went to the hallway.\\n',\n",
       " '3 Where is Mary? \\tbathroom\\t1\\n',\n",
       " '4 Daniel went back to the hallway.\\n',\n",
       " '5 Sandra moved to the garden.\\n',\n",
       " '6 Where is Daniel? \\thallway\\t4\\n',\n",
       " '7 John moved to the office.\\n',\n",
       " '8 Sandra journeyed to the bathroom.\\n',\n",
       " '9 Where is Daniel? \\thallway\\t4\\n',\n",
       " '10 Mary moved to the hallway.\\n']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', 'moved', 'to', 'the', 'bathroom.'], ['john', 'went', 'to', 'the', 'hallway.'], ['where', 'is', 'mary?', '\\tbathroom\\t1']]\n"
     ]
    }
   ],
   "source": [
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "print(tokens[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'went', 'to', 'the', 'hallway.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "word2index = {}\n",
    "for ii, word in enumerate(vocab):\n",
    "    word2index[word] = ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "embed_size = 10 \n",
    "\n",
    "embed = (np.random.rand(len(vocab), embed_size) -0.5) * 0.1\n",
    "recurrent = np.eye(embed_size)\n",
    "\n",
    "start = np.zeros(embed_size)\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1\n",
    "one_hot = np.eye(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent):\n",
    "    \n",
    "    layers = list() \n",
    "    layer = {}\n",
    "    \n",
    "    layer[\"hidden\"] = start\n",
    "    layers.append(layer)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for target_i in range(len(sent)):\n",
    "        \n",
    "        layer = {}\n",
    "        layer[\"pred\"] = softmax(layers[-1][\"hidden\"].dot(decoder))\n",
    "        loss += -np.log(layer[\"pred\"][sent[target_i]])\n",
    "        \n",
    "        layer[\"hidden\"] = layers[-1][\"hidden\"].dot(recurrent) + embed[sent[target_i]]\n",
    "        layers.append(layer)\n",
    "    return layers, loss\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'hidden': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}, {'pred': array([0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512, 0.01219512, 0.01219512, 0.01219512,\n",
      "       0.01219512, 0.01219512]), 'hidden': array([ 0.03666083,  0.0449806 ,  0.0326407 ,  0.03541154, -0.04012566,\n",
      "        0.01513043,  0.0203517 ,  0.01102408,  0.02996153, -0.04654288])}, {'pred': array([0.01214899, 0.01221913, 0.0122163 , 0.01214254, 0.01219559,\n",
      "       0.01218802, 0.01225171, 0.01219535, 0.01222961, 0.01214487,\n",
      "       0.01224788, 0.01224333, 0.01216564, 0.01223319, 0.01225324,\n",
      "       0.01224021, 0.01221367, 0.01215115, 0.01218007, 0.01219157,\n",
      "       0.01217394, 0.01223678, 0.01216535, 0.01224643, 0.01223622,\n",
      "       0.01226338, 0.01217312, 0.01217544, 0.01216391, 0.0121463 ,\n",
      "       0.01212129, 0.01218137, 0.01220884, 0.01219125, 0.01216087,\n",
      "       0.01227842, 0.01218747, 0.01221735, 0.01213751, 0.01220311,\n",
      "       0.01219273, 0.01214382, 0.01222232, 0.01227528, 0.01217557,\n",
      "       0.01220225, 0.01217357, 0.01217755, 0.01215571, 0.01224363,\n",
      "       0.01214777, 0.01212834, 0.01214501, 0.01208895, 0.01220842,\n",
      "       0.01218716, 0.01218752, 0.01222649, 0.01222448, 0.01223221,\n",
      "       0.01221628, 0.01216565, 0.0121628 , 0.01217147, 0.01220819,\n",
      "       0.0121995 , 0.0122182 , 0.01215468, 0.0122583 , 0.01212998,\n",
      "       0.01221396, 0.01224909, 0.01217739, 0.01222412, 0.01220785,\n",
      "       0.01218529, 0.01218783, 0.01221961, 0.01223052, 0.01214521,\n",
      "       0.01225485, 0.01216001]), 'hidden': array([ 0.07268453,  0.05087098,  0.02296275,  0.06128624, -0.01843276,\n",
      "        0.06386305, -0.0018398 , -0.03859655,  0.07335179, -0.01075317])}, {'pred': array([0.01216254, 0.01219087, 0.01222013, 0.0121599 , 0.01212118,\n",
      "       0.01212812, 0.01225136, 0.01219464, 0.01225568, 0.01211545,\n",
      "       0.01223781, 0.01223878, 0.01213732, 0.01222951, 0.01225752,\n",
      "       0.01228965, 0.01217155, 0.0121704 , 0.01218003, 0.0121665 ,\n",
      "       0.0121712 , 0.01220681, 0.01211043, 0.01226872, 0.01222634,\n",
      "       0.01230902, 0.01221687, 0.01214354, 0.01218415, 0.01215586,\n",
      "       0.01212446, 0.01223051, 0.01223108, 0.0121937 , 0.01210217,\n",
      "       0.01229462, 0.0122645 , 0.01216699, 0.01214098, 0.01218391,\n",
      "       0.01216241, 0.0121932 , 0.01222785, 0.01232755, 0.0121893 ,\n",
      "       0.01219959, 0.01218785, 0.01216377, 0.01206385, 0.01223441,\n",
      "       0.01211634, 0.0120625 , 0.01215406, 0.01202091, 0.01223908,\n",
      "       0.01214849, 0.01216217, 0.01219466, 0.01218875, 0.01228937,\n",
      "       0.01222517, 0.01218586, 0.01214138, 0.01220315, 0.01218849,\n",
      "       0.01224063, 0.0121746 , 0.01211182, 0.01224495, 0.01217787,\n",
      "       0.01225683, 0.01223487, 0.012185  , 0.01229841, 0.01221442,\n",
      "       0.01223081, 0.01225591, 0.01224476, 0.01224323, 0.01219243,\n",
      "       0.01221938, 0.01219915]), 'hidden': array([ 0.0335864 ,  0.06424966,  0.05325907,  0.08096629,  0.00818838,\n",
      "        0.04810846,  0.03274535, -0.04571967,  0.10575277,  0.00189645])}, {'pred': array([0.01215327, 0.01218178, 0.01221957, 0.01216737, 0.0121492 ,\n",
      "       0.01213884, 0.01226622, 0.01220976, 0.01228698, 0.01212075,\n",
      "       0.0122728 , 0.01227034, 0.01214165, 0.01220454, 0.01224524,\n",
      "       0.01226554, 0.01216937, 0.01215405, 0.01218471, 0.01218092,\n",
      "       0.01217356, 0.0121655 , 0.01208606, 0.0122553 , 0.01218148,\n",
      "       0.01231414, 0.01222833, 0.01216702, 0.01217639, 0.01219869,\n",
      "       0.0120839 , 0.01221952, 0.01221549, 0.01223168, 0.01205647,\n",
      "       0.01230336, 0.01226076, 0.01215542, 0.01217254, 0.01218217,\n",
      "       0.01214081, 0.01215479, 0.01226321, 0.01234364, 0.01219528,\n",
      "       0.01217993, 0.01219332, 0.0121501 , 0.01205725, 0.01222801,\n",
      "       0.01209413, 0.0120169 , 0.01217457, 0.01201472, 0.01226069,\n",
      "       0.01218329, 0.01216939, 0.01224655, 0.01217732, 0.01229484,\n",
      "       0.01220828, 0.0121874 , 0.01210779, 0.01221669, 0.01213445,\n",
      "       0.01224692, 0.01214092, 0.01213613, 0.01222511, 0.01220597,\n",
      "       0.01230757, 0.01226009, 0.01218697, 0.01228974, 0.01217604,\n",
      "       0.01225902, 0.01223276, 0.01225451, 0.01221825, 0.01221619,\n",
      "       0.01224339, 0.01219635]), 'hidden': array([ 0.02550586,  0.08277161,  0.0237043 ,  0.11877803, -0.03907286,\n",
      "        0.06515521,  0.02447583, -0.03985069,  0.06979147, -0.0282934 ])}, {'pred': array([0.01210758, 0.01220838, 0.0122207 , 0.01216667, 0.01211094,\n",
      "       0.01215075, 0.01226015, 0.01223543, 0.01227704, 0.01209241,\n",
      "       0.01225572, 0.01222999, 0.01216478, 0.01228113, 0.01228538,\n",
      "       0.01230132, 0.0122139 , 0.01212317, 0.01213697, 0.01216036,\n",
      "       0.01215344, 0.01221466, 0.01210443, 0.01226679, 0.01225187,\n",
      "       0.0123651 , 0.01218427, 0.01216991, 0.01217948, 0.01216097,\n",
      "       0.01204921, 0.01221041, 0.01225882, 0.01224435, 0.01208363,\n",
      "       0.01230998, 0.01218945, 0.01216966, 0.01214235, 0.01217179,\n",
      "       0.01218085, 0.01218658, 0.01223513, 0.0123718 , 0.01214665,\n",
      "       0.01213238, 0.01217823, 0.01212907, 0.01206902, 0.01223214,\n",
      "       0.01208968, 0.01204067, 0.0121569 , 0.01199983, 0.01221838,\n",
      "       0.01215411, 0.01222047, 0.01225468, 0.01220809, 0.01230613,\n",
      "       0.01223001, 0.01219041, 0.01209455, 0.01219328, 0.01215407,\n",
      "       0.01224025, 0.01219202, 0.01213355, 0.01225713, 0.01214524,\n",
      "       0.01228897, 0.01229437, 0.01221352, 0.01230166, 0.01215427,\n",
      "       0.0122248 , 0.01220546, 0.01227   , 0.01222509, 0.01217698,\n",
      "       0.01228387, 0.01215637]), 'hidden': array([-0.00726009,  0.04648518,  0.06696384,  0.13845985, -0.08247284,\n",
      "        0.09070152,  0.04986345,  0.00245176,  0.09094394, -0.06586631])}]\n"
     ]
    }
   ],
   "source": [
    "layer, loss = predict(word2indices(tokens[1]))\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:82.05801257706915\n",
      "Perplexity:81.7990793441101\n",
      "Perplexity:81.42272196378181\n",
      "Perplexity:80.69903225946521\n",
      "Perplexity:78.98693438589551\n",
      "Perplexity:73.08526905400181\n",
      "Perplexity:34.691344532264374\n",
      "Perplexity:21.834262201639888\n",
      "Perplexity:21.36056083292534\n",
      "Perplexity:20.798918847005446\n",
      "Perplexity:20.195492452086448\n",
      "Perplexity:19.51138466202749\n",
      "Perplexity:18.758623223580457\n",
      "Perplexity:18.09651341403817\n",
      "Perplexity:17.532642141254286\n",
      "Perplexity:16.947618398556735\n",
      "Perplexity:16.467411467075777\n",
      "Perplexity:16.08121997874825\n",
      "Perplexity:15.728543664270317\n",
      "Perplexity:15.422030818687233\n",
      "Perplexity:14.012238135413842\n",
      "Perplexity:13.624185010349143\n",
      "Perplexity:13.894423778320744\n",
      "Perplexity:12.88620425377288\n",
      "Perplexity:12.120166794194914\n",
      "Perplexity:10.591269448030882\n",
      "Perplexity:7.993765313467362\n",
      "Perplexity:7.625368992575441\n",
      "Perplexity:6.808815460716412\n",
      "Perplexity:7.156277969956574\n"
     ]
    }
   ],
   "source": [
    "#Backprop and update of RNN\n",
    "for iter in range(30000):\n",
    "    alpha = 0.001\n",
    "    #Indices for every second to end word in a sentence.\n",
    "    sent = word2indices(tokens[iter%len(tokens)][1:])\n",
    "    layers, loss = predict(sent)\n",
    "    \n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        \n",
    "        layer = layers[layer_idx] # The current timestep\n",
    "        target = sent[layer_idx - 1] #The previous timestep's word index\n",
    "        \n",
    "        #Not first timestep\n",
    "        if layer_idx > 0: \n",
    "            #Calculate the error of the output decoder\n",
    "            layer[\"output_delta\"] = layer[\"pred\"] - one_hot[target]\n",
    "            #Find error of each hidden dimension\n",
    "            new_hidden_delta = layer[\"output_delta\"].dot(decoder.T)\n",
    "            if (layer_idx == len(layers) - 1):\n",
    "            #If the last timestep then there are no next timesteps\n",
    "                layer[\"hidden_delta\"] = new_hidden_delta\n",
    "            else:\n",
    "            #These changes are also affected by the delta from the next timesteps\n",
    "                layer[\"hidden_delta\"] = new_hidden_delta + layers[layer_idx + 1][\"hidden_delta\"].dot(recurrent.T)\n",
    "                \n",
    "        else:\n",
    "            #This hidden dimension is the init hidden has not been used to predict an output so no output delta\n",
    "            layer[\"hidden_delta\"] = layers[layer_idx+1][\"hidden_delta\"].dot(recurrent.T)\n",
    "            \n",
    "    \n",
    "    #Only update the hidden dimension layer\n",
    "    start -= layers[0][\"hidden_delta\"] * alpha / float(len(sent))\n",
    "    for layer_idx,layer in enumerate(layers[1:]):\n",
    "        \n",
    "        #Here the output delta is error and the hidden layers is input like FFNN\n",
    "        decoder -= np.outer(layers[layer_idx][\"hidden\"], layer[\"output_delta\"]) * alpha /float(len(sent))\n",
    "        \n",
    "        embed_idx = sent[layer_idx]\n",
    "        #The embedding is the amount I change the network it is subtracting the hidden delta\n",
    "        embed[embed_idx] -= layers[layer_idx][\"hidden_delta\"] * alpha / float(len(sent))\n",
    "        #The recurrent weights are the hidden later \n",
    "        recurrent -= np.outer(layers[layer_idx][\"hidden\"], layers[layer_idx][\"hidden_delta\"]) * alpha/ float(len(sent))\n",
    "    \n",
    "    if(iter % 1000 == 0):\n",
    "        print(\"Perplexity:\" + str(np.exp(loss/len(sent))))\n",
    "    \n",
    "           \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating layers[state]\n",
    "# If the timestep is the first timestep\n",
    "# If yes: then no output layer so instead just allocated to each hidden node in hidden dimension\n",
    "# If no:\n",
    "    #If it is the last timestep:\n",
    "        #Find output delta only then allocated that to each hidden dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
